{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleja\\AppData\\Local\\Temp\\ipykernel_6196\\2481162812.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd #dataframes\n",
      "[nltk_data] Downloading package udhr to\n",
      "[nltk_data]     C:\\Users\\aleja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package udhr is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Packages\n",
    "import pandas as pd #dataframes\n",
    "import numpy as np #for arrays \n",
    "\n",
    "#NLP libraries\n",
    "import nltk \n",
    "from nltk.corpus import udhr #corpora with texts \n",
    "import re #Regular expressions\n",
    "import spacy \n",
    "import es_core_news_sm, ko_core_news_sm, fi_core_news_sm, zh_core_web_sm, ja_core_news_sm, pl_core_news_sm, de_core_news_sm #spacy models\n",
    "\n",
    "nltk.download('udhr')\n",
    "\n",
    "#NLP objects for (as we can't use shortcuts for loading the objects)\n",
    "nlp_es= spacy.load(\"es_core_news_sm\") #Spanish\n",
    "nlp_ko= spacy.load(\"ko_core_news_sm\") #Korean\n",
    "nlp_fi= spacy.load(\"fi_core_news_sm\") #Finnish\n",
    "nlp_zh= spacy.load(\"zh_core_web_sm\") #Chinese\n",
    "nlp_ja= spacy.load(\"ja_core_news_sm\") #Japanese\n",
    "nlp_pl= spacy.load(\"pl_core_news_sm\") #Polish\n",
    "nlp_de= spacy.load(\"de_core_news_sm\") #German\n",
    "\n",
    "#other spacy models for less explored languages \n",
    "from spacy.lang.tr import Turkish\n",
    "nlp_tr= Turkish()\n",
    "from spacy.lang.id import Indonesian\n",
    "nlp_id= Indonesian()\n",
    "from spacy.lang.ar import Arabic\n",
    "nlp_ar= Arabic()\n",
    "from spacy.lang.tl import Tagalog\n",
    "nlp_tl= Tagalog()\n",
    "from spacy.lang.eu import Basque\n",
    "nlp_eu= Basque()\n",
    "from spacy.lang.et import Estonian\n",
    "nlp_et= Estonian()\n",
    "from spacy.lang.kn import Kannada\n",
    "nlp_kn= Kannada()\n",
    "from spacy.lang.yo import Yoruba \n",
    "nlp_yo= Yoruba()\n",
    "from spacy.lang.sk import Slovak\n",
    "nlp_sk= Slovak()\n",
    "from spacy.lang.ms import Malay\n",
    "nlp_ms= Malay()\n",
    "from spacy.lang.ga import Irish\n",
    "nlp_ga= Irish()\n",
    "from spacy.lang.tn import Setswana\n",
    "nlp_tn= Setswana()\n",
    "from spacy.lang.bg import Bulgarian\n",
    "nlp_bg= Bulgarian()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Language</th>\n",
       "      <th>Family</th>\n",
       "      <th>ISO code</th>\n",
       "      <th>tokens</th>\n",
       "      <th>types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>Austronesian</td>\n",
       "      <td>ar</td>\n",
       "      <td>1318</td>\n",
       "      <td>725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>Basque</td>\n",
       "      <td>N/D</td>\n",
       "      <td>eu</td>\n",
       "      <td>1236</td>\n",
       "      <td>652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>Bulgarian</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>bg</td>\n",
       "      <td>2273</td>\n",
       "      <td>653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>zh</td>\n",
       "      <td>2693</td>\n",
       "      <td>532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>Estonian</td>\n",
       "      <td>Uralic</td>\n",
       "      <td>et</td>\n",
       "      <td>1250</td>\n",
       "      <td>654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>Finnish</td>\n",
       "      <td>Uralic</td>\n",
       "      <td>fi</td>\n",
       "      <td>1113</td>\n",
       "      <td>672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14</td>\n",
       "      <td>German</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>de</td>\n",
       "      <td>1330</td>\n",
       "      <td>545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>Indonesian</td>\n",
       "      <td>Austronesian</td>\n",
       "      <td>id</td>\n",
       "      <td>1302</td>\n",
       "      <td>488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>19</td>\n",
       "      <td>Irish</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>ga</td>\n",
       "      <td>1640</td>\n",
       "      <td>598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>Japonic</td>\n",
       "      <td>ja</td>\n",
       "      <td>2325</td>\n",
       "      <td>517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Kannada</td>\n",
       "      <td>Dravian</td>\n",
       "      <td>kn</td>\n",
       "      <td>401</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>Korean</td>\n",
       "      <td>Koreanic</td>\n",
       "      <td>ko</td>\n",
       "      <td>996</td>\n",
       "      <td>557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>18</td>\n",
       "      <td>Malay</td>\n",
       "      <td>Austronesian</td>\n",
       "      <td>ms</td>\n",
       "      <td>1288</td>\n",
       "      <td>462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>Polish</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>pl</td>\n",
       "      <td>1334</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16</td>\n",
       "      <td>Setswana</td>\n",
       "      <td>Atlantic-Congo</td>\n",
       "      <td>tn</td>\n",
       "      <td>1735</td>\n",
       "      <td>459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17</td>\n",
       "      <td>Slovak</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>sk</td>\n",
       "      <td>1444</td>\n",
       "      <td>726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>es</td>\n",
       "      <td>1559</td>\n",
       "      <td>509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8</td>\n",
       "      <td>Tagalog</td>\n",
       "      <td>Afro-Asiatic</td>\n",
       "      <td>tl</td>\n",
       "      <td>1509</td>\n",
       "      <td>453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>Turkic</td>\n",
       "      <td>tr</td>\n",
       "      <td>1310</td>\n",
       "      <td>696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>12</td>\n",
       "      <td>Yoruba</td>\n",
       "      <td>Atlantic-Congo</td>\n",
       "      <td>yo</td>\n",
       "      <td>1437</td>\n",
       "      <td>385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0    Language          Family ISO code  tokens  types\n",
       "0            7      Arabic    Austronesian       ar    1318    725\n",
       "1            9      Basque             N/D       eu    1236    652\n",
       "2           15   Bulgarian   Indo-European       bg    2273    653\n",
       "3            5     Chinese    Sino-Tibetan       zh    2693    532\n",
       "4           10    Estonian          Uralic       et    1250    654\n",
       "5            2     Finnish          Uralic       fi    1113    672\n",
       "6           14      German   Indo-European       de    1330    545\n",
       "7            4  Indonesian    Austronesian       id    1302    488\n",
       "8           19       Irish   Indo-European       ga    1640    598\n",
       "9            6    Japanese         Japonic       ja    2325    517\n",
       "10          11     Kannada         Dravian       kn     401    304\n",
       "11           1      Korean        Koreanic       ko     996    557\n",
       "12          18       Malay    Austronesian       ms    1288    462\n",
       "13          13      Polish   Indo-European       pl    1334    655\n",
       "14          16    Setswana  Atlantic-Congo       tn    1735    459\n",
       "15          17      Slovak   Indo-European       sk    1444    726\n",
       "16           0     Spanish   Indo-European       es    1559    509\n",
       "17           8     Tagalog    Afro-Asiatic       tl    1509    453\n",
       "18           3     Turkish          Turkic       tr    1310    696\n",
       "19          12      Yoruba  Atlantic-Congo       yo    1437    385"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root= \"D:/CCiL/Quantitative Linguistics/lab_22\"\n",
    "df= pd.read_csv(root + \"/language_data_2.csv\", sep=\",\")\n",
    "languages= df['Language'].values\n",
    "codes= df['ISO code'].values\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_raw_texts(list_of_languages, list_of_codes):\n",
    "  raw_files_names= {}\n",
    "  for language in list_of_languages:\n",
    "    for code in list_of_codes:\n",
    "        all_files= nltk.corpus.udhr.fileids()\n",
    "        file= [f for f in all_files if re.findall(language, f)]\n",
    "        raw= nltk.corpus.udhr.raw(file)   \n",
    "        raw_files_names[language]=raw\n",
    "  return raw_files_names\n",
    "\n",
    "def tokenizer(text, model_lang):\n",
    "    nlp= model_lang #Opens spacy object\n",
    "    doc=nlp(text) #Process text with spacy \n",
    "    tokens = [token.text for token in doc if not token.is_space and not token.is_punct and not token.is_digit]\n",
    "    return tokens\n",
    "\n",
    "def tokens(dict_raw_texts, languages): #takes real_tokenizer and filters by language to tokenize\n",
    "    tokens_langs= {} #dictionary to store output\n",
    "    for lang in languages:\n",
    "        if lang == 'Spanish':\n",
    "            text= dict_raw_texts[lang] #gets text from dict in raw_files_names \n",
    "            model_lang= nlp_es #loads corresponding model\n",
    "            tokens= tokenizer(text, model_lang) #tokenizes \n",
    "            tokens_langs[lang]=tokens #appends to output dictionary \n",
    "        elif lang == 'Korean':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_ko\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Finnish':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_fi\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Chinese':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_zh\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Japanese':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_ja\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Polish':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_pl\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'German':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_de\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Turkish':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_tr\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Indonesian':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_id\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Arabic':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_ar\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Tagalog':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_tl\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Basque':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_eu\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Estonian':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_et\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Kannada':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_kn\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Yoruba':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_yo\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Malay':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_ms\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Slovak':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_sk\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Setswana':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_tn\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Bulgarian':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_bg\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "        elif lang == 'Irish':\n",
    "            text= dict_raw_texts[lang]\n",
    "            model_lang= nlp_ga\n",
    "            tokens= tokenizer(text, model_lang)\n",
    "            tokens_langs[lang]=tokens\n",
    "    return tokens_langs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_texts= extract_raw_texts(languages, codes) #returns a dictionary where KEY is language and VALUE a string with raw text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ar Done!\n",
      "eu Done!\n",
      "bg Done!\n",
      "zh Done!\n",
      "et Done!\n",
      "fi Done!\n",
      "de Done!\n",
      "id Done!\n",
      "ga Done!\n",
      "ja Done!\n",
      "kn Done!\n",
      "ko Done!\n",
      "ms Done!\n",
      "pl Done!\n",
      "tn Done!\n",
      "sk Done!\n",
      "es Done!\n",
      "tl Done!\n",
      "tr Done!\n",
      "yo Done!\n"
     ]
    }
   ],
   "source": [
    "#Writing files in .txt files \n",
    "for language, code in zip(languages, codes):\n",
    "    file_name = root + \"/data/\" + str(code) + \".txt\"\n",
    "    text = str(raw_texts[language])  # Fetch text for the current language\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text)\n",
    "    print(f'{code} Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "all_languages_tokens= tokens(raw_texts, languages) #returns a dictionary where KEY is language and VALUE is list with tokens.\n",
    "print(len(all_languages_tokens)) #just for checking how many variables have been processed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.sort_values('Language')\n",
    "languages= df['Language'].values\n",
    "tokens_languages=[]\n",
    "types_languages=[]\n",
    "for i in languages:\n",
    "    txt= all_languages_tokens[i]\n",
    "    tokens_languages.append(len(txt))\n",
    "    types_languages.append(len(set(txt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens']=tokens_languages\n",
    "df['types']=types_languages\n",
    "\n",
    "df.to_csv('language_data_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "def process(tokens):\n",
    "    token_freq = Counter(tokens)\n",
    "    matrix = []\n",
    "    for token in set(tokens):\n",
    "        matrix.append([token, len(token), token_freq[token]])\n",
    "    \n",
    "    matrix.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "tokens_langs= {}\n",
    "for language in all_languages_tokens:\n",
    "    tokens_langs[language] = process(all_languages_tokens[language])\n",
    "\n",
    "def matrix_to_csv(matrix, filename):\n",
    "    with open(filename, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        # Write each row of the matrix to the CSV file\n",
    "        writer.writerow(['Token', 'Length', 'Frequency'])\n",
    "        for row in matrix:\n",
    "            writer.writerow(row)\n",
    "\n",
    "for language in tokens_langs:\n",
    "    filename = f\"data/output_{language.lower()}.csv\"\n",
    "    matrix_to_csv(tokens_langs[language], filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
